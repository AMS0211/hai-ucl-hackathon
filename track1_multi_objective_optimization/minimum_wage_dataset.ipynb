{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holistic AI x UCL AI Society Hackathon Tutorial\n",
    "\n",
    "### Track 1: Multi-Objective Optimization for AI Trustworthiness in Tabular Data Classification\n",
    "\n",
    "<h1 align=\"center\">\n",
    "<img src=\"https://bmcnews.com.br/wp-content/uploads/2023/11/NOVO-PROJETO-27-1024x536.jpg\" width=\"500\">\n",
    "<br>Study Case: The Minimum Wage Dataset\n",
    "<h4 align=\"center\">Technical Risks in Machine Learning Models</h2>\n",
    "</h1>\n",
    "\n",
    "\n",
    "The **Minimum Wage** dataset, specifically curated for the first edition of the HAI-UCL Hackathon, originates from Brazil's **RAIS** (Relação Anual de Informações Sociais) database. Compiled annually by the Brazilian Ministry of Labor, RAIS provides detailed records on formal employment, encompassing essential information such as worker demographics, wage levels, job tenure, and firm-specific details like location, industry classification, and company size. RAIS data enables rigorous labor economic analyses, making it crucial for studying income distribution, employment patterns, and social inequalities in Brazil.\n",
    "\n",
    "The **Minimum Wage** dataset includes a carefully balanced sample of 100,000 workers, divided equally across four demographic groups: White-Male, White-Female, Black-Male, and Black-Female (each comprising 25%). This balanced design allows for focused analysis of wage disparities and demographic influences by race and gender, supporting insights into socioeconomic trends in the Brazilian labor market.\n",
    "\n",
    "**(Goal)** The primary goal of analyzing the Minimum Wage dataset is to predict whether a worker’s average income falls above or below five times the minimum wage threshold.\n",
    "\n",
    "**(Features)** The following table presents the 13 features of Minimum Wage dataset, including the target variable \"y (target)\".\n",
    "\n",
    "| Feature                  | Description                                                                                      |\n",
    "|--------------------------|--------------------------------------------------------------------------------------------------|\n",
    "| `age` | Age of the individual in years. |\n",
    "| `state` | Federal state of residence or work, typically coded as a two-letter abbreviation (e.g., RJ for Rio de Janeiro, SP for São Paulo). |\n",
    "| `time_employed` | Duration of employment with the current employer, often measured in months or years. |\n",
    "| `contracted_hours` | Contracted weekly working hours, indicating the standard hours agreed upon for the role. |\n",
    "| `intermittent_work` | Indicator of intermittent work status (binary: 0 = \"No\", 1 = \"Yes\"), denoting sporadic or flexible employment arrangements. |\n",
    "| `partial_time_work` | Indicator of part-time employment (binary: 0 = \"No\", 1 = \"Yes\"), typically under 40 hours per week. |\n",
    "| `economic_sector` | Categorical field for the sector of the economy in which the individual is employed (e.g., agriculture, manufacturing). |\n",
    "| `cbo_2002` | Occupational classification code based on the Brazilian Classification of Occupations (CBO 2002). |\n",
    "| `age_group` | Categorical age group classification (e.g., 18-24, 25-34), often used for demographic analysis. |\n",
    "| `education_level` | Highest level of education completed (e.g., primary, secondary, higher education). |\n",
    "| `sex` | Sex of the individual (binary: \"Male\", \"Female\"). |\n",
    "| `race` | Race/ethnicity classification, typically based on self-identification categories (e.g., White, Black, Pardo). |\n",
    "| `disability` | Indicator of disability status (binary: 0 = \"No\", 1 = \"Yes\"), identifying individuals with disabilities. |\n",
    "| `y (target)` | Target variable indicating whether the worker receives more than five minimum wages on average (binary: 0 = \"No\", 1 = \"Yes\"). |"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup and Dependencies\n",
    "Install the necessary libraries to get started:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: holisticai in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (1.0.9)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (1.5.2)\r\n",
      "Requirement already satisfied: pandas in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (2.0.2)\r\n",
      "Requirement already satisfied: matplotlib in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (3.9.2)\r\n",
      "Requirement already satisfied: seaborn in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (0.13.2)\r\n",
      "Requirement already satisfied: jax in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (0.4.35)\r\n",
      "Requirement already satisfied: networkx>=3.1 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from holisticai) (3.4.2)\r\n",
      "Requirement already satisfied: pybind11>=2.12 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from holisticai) (2.13.6)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from pandas) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from pandas) (2024.2)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from matplotlib) (1.3.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from matplotlib) (4.54.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from matplotlib) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from matplotlib) (24.1)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from matplotlib) (11.0.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from matplotlib) (3.2.0)\r\n",
      "Requirement already satisfied: jaxlib<=0.4.35,>=0.4.34 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from jax) (0.4.35)\r\n",
      "Requirement already satisfied: ml-dtypes>=0.4.0 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from jax) (0.5.0)\r\n",
      "Requirement already satisfied: opt-einsum in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from jax) (3.4.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/zekunwu/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install holisticai scikit-learn pandas numpy matplotlib seaborn jax"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T17:06:06.982912Z",
     "start_time": "2024-11-18T17:06:05.286384Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T17:06:12.835127Z",
     "start_time": "2024-11-18T17:06:12.819162Z"
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mholisticai\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset\n\u001B[0;32m----> 3\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmw_small\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotected_attribute\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrace\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m split_dataset \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mtrain_test_split(test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m      5\u001B[0m train \u001B[38;5;241m=\u001B[39m split_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/Desktop/HAI-UCL-Hackathon/.venv/lib/python3.10/site-packages/holisticai/datasets/_load_dataset.py:979\u001B[0m, in \u001B[0;36mload_dataset\u001B[0;34m(dataset_name, preprocessed, protected_attribute, target)\u001B[0m\n\u001B[1;32m    977\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dataset_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124macspublic\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    978\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_acspublic_dataset(preprocessed\u001B[38;5;241m=\u001B[39mpreprocessed, protected_attribute\u001B[38;5;241m=\u001B[39mprotected_attribute)\n\u001B[0;32m--> 979\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from holisticai.datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mw_small\", protected_attribute=\"race\")\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, random_state=42)\n",
    "train = split_dataset['train']\n",
    "test = split_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train a Binary Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T17:06:07.301172Z",
     "start_time": "2024-11-18T17:06:07.301112Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from holisticai.utils import BinaryClassificationProxy\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model.fit(train['X'], train['y'])\n",
    "\n",
    "proxy = BinaryClassificationProxy(predict=model.predict, predict_proba=model.predict_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Measuring Efficacy\n",
    "\n",
    "Efficacy metrics are used to evaluate the performance of a model by quantifying how accurately it makes predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Accuracy\n",
    "\n",
    "**Accuracy** is a metric that measures the proportion of correct predictions made by a classification model out of the total predictions. It is a commonly used metric for model evaluation, especially when the classes are balanced.\n",
    "\n",
    "### Formula\n",
    "\n",
    "Accuracy is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Predictions}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **True Positives (TP)**: Correct positive predictions.\n",
    "- **True Negatives (TN)**: Correct negative predictions.\n",
    "- **Total Predictions** $= \\text{True Positives} + \\text{True Negatives} + \\text{False Positives} + \\text{False Negatives}$.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Accuracy = 1**: The model predicts all outcomes correctly.\n",
    "- **Accuracy < 1**: The model has some incorrect predictions.\n",
    "\n",
    "Accuracy is useful when the class distribution is balanced. However, it can be misleading if the data is imbalanced, as it does no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T17:06:07.301917Z",
     "start_time": "2024-11-18T17:06:07.301834Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_test_pred = proxy.predict(test['X'])\n",
    "accuracy_score(test['y'], y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) F1 Score\n",
    "\n",
    "The **F1 Score** is a metric that combines both precision and recall into a single score. It is especially useful for evaluating the performance of a classification model when there is an imbalance in the class distribution, as it balances the trade-off between false positives and false negatives.\n",
    "\n",
    "### Formula\n",
    "\n",
    "The F1 Score is the harmonic mean of precision and recall and is defined as:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **Precision**: The ratio of correctly predicted positive observations to total predicted positives.\n",
    "\n",
    "  $$\n",
    "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "  $$\n",
    "\n",
    "- **Recall**: The ratio of correctly predicted positive observations to all actual positives.\n",
    "\n",
    "  $$\n",
    "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "  $$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **F1 Score = 1**: Perfect precision and recall.\n",
    "- **F1 Score = 0**: Model fails to identify any true positives.\n",
    "  \n",
    "A higher F1 Score indicates better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(test['y'], y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What other efficacy metrics do we have for binary classification? Holistic AI library allows you to perform a practical inspection of the metrics by calling ```classification_efficacy_metrics```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.efficacy.metrics import classification_efficacy_metrics\n",
    "\n",
    "classification_efficacy_metrics(test['y'], y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Measuring Bias\n",
    "\n",
    "In machine learning fairness, Equal Opportunity and Equal Outcomes are distinct concepts:\n",
    "\n",
    "- **Equal Opportunity**: Ensures individuals with similar qualifications have equal chances of positive outcomes, focusing on equal true positive rates across groups.\n",
    "\n",
    "- **Equal Outcomes**: Aims for equal proportions of positive results among different demographic groups, regardless of individual qualifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Disparate Impact\n",
    "\n",
    "**Disparate Impact** is a fairness metric used to measure the relative rate of favorable outcomes between an unprivileged group and a privileged group. It is commonly used to assess whether a model’s predictions disproportionately favor one group over another. A value close to 1 indicates fairness, while values significantly above or below 1 indicate potential bias.\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\n",
    "\\text{Disparate Impact} = \\frac{P(\\text{positive outcome | unprivileged})}{P(\\text{positive outcome | privileged})}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $P(\\text{positive outcome | unprivileged})$ is the probability of a positive outcome for the unprivileged group.\n",
    "- $P(\\text{positive outcome | privileged})$ is the probability of a positive outcome for the privileged group.\n",
    "\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Disparate Impact ≈ 1**: The model is fair (similar outcomes for both groups).\n",
    "- **< 1**: The privileged group has a higher rate of positive outcomes.\n",
    "- **> 1**: The unprivileged group has a higher rate of positive outcomes.\n",
    "\n",
    "In practice, a common threshold for fairness is that **Disparate Impact should be between 0.8 and 1.25**. If the metric falls outside this range, the model may be considered biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.bias.metrics import disparate_impact\n",
    "\n",
    "disparate_impact(test['group_a'], test['group_b'], y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Equal Opportunity Difference\n",
    "\n",
    "The **Equal Opportunity Difference** is a fairness metric that measures the difference in true positive rates (TPR) between privileged and unprivileged groups in a classification model. Ideally, this difference should be close to zero to ensure fairness.\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\n",
    "\\text{Equal Opportunity Difference} = TPR_{privileged} - TPR_{unprivileged}\n",
    "$$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **0**: The model is fair (Equal Opportunity).\n",
    "- **Positive**: The privileged group has a higher chance of a positive prediction.\n",
    "- **Negative**: The unprivileged group has a higher chance of a positive prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.bias.metrics import equal_opportunity_diff\n",
    "\n",
    "equal_opportunity_diff(test['group_a'], test['group_b'], y_test_pred, test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access all bias metrics for binary classification available in the HolisticAI library using the ```classification_bias_metrics``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.bias.metrics import classification_bias_metrics\n",
    "\n",
    "classification_bias_metrics(test['group_a'], test['group_b'], y_test_pred, test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Explainability\n",
    "\n",
    "Model interpretability can be quantified through various criteria that assess how understandable a model is, as well as through transparent surrogate models that aim to represent the actual model and its decision-making process. Some criteria include:\n",
    "\n",
    "- **Concentration of Feature Importance**: Evaluating how feature importance is distributed across the model.\n",
    "\n",
    "- **Consistency of Feature Importance Ranking**: Assessing the stability of feature importance rankings across different decision groups within the model, such as classes 0 and 1 in binary classification.\n",
    "\n",
    "- **Stability of Feature Importance Across the Dataset**: Analyzing how feature importance varies throughout the dataset.\n",
    "\n",
    "- **Complexity of Feature Relationships**: Examining the complexity of the relationship between features and their partial dependence plots.\n",
    "\n",
    "Another approach to quantifying interpretability is to evaluate how effectively a transparent model fits the actual model. For instance, if the surrogate model is a decision tree, additional measures such as tree depth or the number of features used by the transparent model can be considered.\n",
    "\n",
    "In this tutorial, we will calculate certain model features that will assist in evaluating interpretability metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.inspection import compute_permutation_importance\n",
    "from holisticai.utils.surrogate_models import create_surrogate_model\n",
    "from holisticai.inspection import compute_partial_dependence\n",
    "from holisticai.inspection import compute_conditional_permutation_importance\n",
    "\n",
    "\n",
    "# Feature Importance\n",
    "importance = compute_permutation_importance(proxy, X=train['X'], y=train['y'])\n",
    "top_importance = importance.top_alpha()\n",
    "top_feature_names = top_importance.feature_names\n",
    "\n",
    "# Conditional Feature Importance\n",
    "conditional_impotance = compute_conditional_permutation_importance(proxy, X=test['X'], y=test['y'])\n",
    "\n",
    "# Partial Dependence\n",
    "X_sample = test['X'].sample(1000)\n",
    "partial_dependence = compute_partial_dependence(features=top_feature_names, proxy=proxy, X=X_sample)\n",
    "\n",
    "# Surrogate Model\n",
    "surrogate_model = create_surrogate_model(X=train['X'], y_pred=train['y'], surrogate_type='tree', learning_task='classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring Alpha  Score that represents the percentage of features that represent the 80% of importnaces in the model behaivours. A lower value points that we can explain the model decision with few features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.explainability.metrics.global_feature_importance import alpha_score\n",
    "\n",
    "alpha_score(importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring Fluctuation Ratio points the percentage in the PDP curve that present fluctuation in the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.explainability.metrics.global_feature_importance import fluctuation_ratio\n",
    "\n",
    "fluctuation_ratio(partial_dependence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank Alignment estimates the percentage of features that remain aligned at the top of importances for label 0 and label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.explainability.metrics.global_feature_importance import rank_alignment\n",
    "\n",
    "rank_alignment(conditional_impotance, top_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Feature Importance Based Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.explainability.metrics import classification_global_feature_importance_explainability_metrics\n",
    "\n",
    "classification_global_feature_importance_explainability_metrics(partial_dependence, importance, conditional_impotance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree based Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.explainability.metrics import tree_explainability_metrics\n",
    "\n",
    "tree_explainability_metrics(surrogate_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surrogate Based Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.explainability.metrics import classification_surrogate_explainability_metrics\n",
    "\n",
    "y_pred = proxy.predict(test['X'])\n",
    "classification_surrogate_explainability_metrics(test['X'], test['y'], y_pred, surrogate_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Security metrics in machine learning evaluate the robustness and resilience of models against threats to data privacy, model integrity, and information leakage. They provide insight into how susceptible a model is to various risks and guide strategies to safeguard sensitive information and maintain reliable performance.\n",
    "\n",
    "Here’s a brief description of each type of security metric in the library:\n",
    "\n",
    "- **SHAPR**: This metric assesses membership privacy risk by estimating how much individual training data points influence model predictions. By using Shapley values, SHAPR quantifies the likelihood of data memorization, which can reveal vulnerabilities to membership inference attacks.\n",
    "\n",
    "- **Data Minimizer**: Similar to feature selection methods, Data Minimizer evaluates the model’s performance as features are iteratively removed. By identifying features that have minimal impact on model accuracy, it highlights areas where data minimization can improve privacy without sacrificing effectiveness.\n",
    "\n",
    "- **Privacy Risk Score**: This metric quantifies the likelihood that a specific input sample originates from the model's training set, based on the model's responses to that sample. Privacy Risk Score helps to identify which samples present the highest risk to be classified as training data, or in general, how risky is the model in terms of data privacy with respect to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.security.commons import DataMinimizer\n",
    "from holisticai.security.metrics import data_minimization_score\n",
    "\n",
    "dmin = DataMinimizer(proxy=proxy, selector_types=[\"Percentile\", \"Variance\"])\n",
    "dmin.fit(train['X'], train['y'])\n",
    "\n",
    "y_pred_test = proxy.predict(test['X'])\n",
    "y_pred_test_dm = dmin.predict(test['X'])\n",
    "\n",
    "\n",
    "data_minimization_score(test['y'], y_pred_test, y_pred_test_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.security.metrics import privacy_risk_score\n",
    "\n",
    "target_shadow = train.train_test_split(test_size=0.4, random_state=42)\n",
    "\n",
    "X_shadow_train = target_shadow['test']['X']\n",
    "y_shadow_train = target_shadow['test']['y']\n",
    "\n",
    "shadow_model = RandomForestClassifier(random_state=42)\n",
    "shadow_model.fit(X_shadow_train, y_shadow_train)\n",
    "\n",
    "shadow_train_probs = shadow_model.predict_proba(X_shadow_train)\n",
    "shadow_test_probs = shadow_model.predict_proba(test['X'])\n",
    "target_train_probs = proxy.predict_proba(train['X'])\n",
    "target_test_probs = proxy.predict_proba(test['X'])\n",
    "\n",
    "risk_score_train = privacy_risk_score((shadow_train_probs, y_shadow_train), (shadow_test_probs, test['y']), (target_train_probs, train['y']))\n",
    "print(\"Mean Privacy Risk Score for train: \", risk_score_train.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this metric estimates an individual sample’s probability of being in the target model’s training set. By calculating the mean Privacy Risk Score across all samples, we can obtain a global measure of the model’s privacy risk. In an ideal scenario, the Privacy Risk Score should be close to 0, indicating a low risk of data memorization, while higher values suggest potential vulnerabilities to membership inference attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Robustness\n",
    "\n",
    "Robustness metrics are measures used to evaluate how well machine learning models maintain performance under varying conditions, such as data shifts, adversarial inputs, or environmental changes. These metrics help assess a model’s resilience and reliability in real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Degradation Factor (ADF) is a robustness metric that detects the first point of significant accuracy drop as a dataset is gradually reduced, allowing for the prediction of performance shifts due to changing data distributions. Here is an exampel how to use the metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.robustness.metrics import (\n",
    "    accuracy_degradation_profile,\n",
    "    accuracy_degradation_factor,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "results = accuracy_degradation_profile(test['X'], \n",
    "                                    test['y'], \n",
    "                                    y_test_pred, \n",
    "                                    n_neighbors = 50,\n",
    "                                    step_size = 0.02,\n",
    "                                    )\n",
    "accuracy_degradation_factor(pd.DataFrame(results.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Example: Bias-Accuracy Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from holisticai.bias.metrics import equal_opportunity_diff, statistical_parity, disparate_impact\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseMetric:\n",
    "    # BaseMetric class with typical inputs\n",
    "    def __init__(self, model, train, test):\n",
    "        self.model = model\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "\n",
    "class MyCustomMetric(BaseMetric):\n",
    "    best_value = 0\n",
    "    # You can define your own custom metric to be optimized                \n",
    "    def compute(self):\n",
    "        y_pred = self.model.predict(self.test['X'])\n",
    "        acc = accuracy_score(self.test['y'], y_pred)\n",
    "        di = disparate_impact(self.test['group_a'], self.test['group_b'], y_pred)\n",
    "        \n",
    "        error_accuracy = 1 - acc # min value: 0\n",
    "        error_bias = abs(1-di) # min value: 0\n",
    "        error  = error_accuracy + error_bias\n",
    "        return error\n",
    "\n",
    "class EqualOpportunity(BaseMetric):\n",
    "    best_value = 0\n",
    "    def compute(self):\n",
    "        y_pred = self.model.predict(self.test['X'])\n",
    "        eo = equal_opportunity_diff(self.test['group_a'], self.test['group_b'], y_pred, self.test['y'])\n",
    "        return eo\n",
    "\n",
    "class Accuracy(BaseMetric):\n",
    "    best_value = 1\n",
    "    def compute(self):\n",
    "        y_pred = self.model.predict(self.test['X'])\n",
    "        eo = accuracy_score(self.test['y'], y_pred)\n",
    "        return eo\n",
    "\n",
    "class DisparateImpact(BaseMetric):\n",
    "    best_value = 1\n",
    "    def compute(self):\n",
    "        y_pred = self.model.predict(self.test['X'])\n",
    "        di = disparate_impact(self.test['group_a'], self.test['group_b'], y_pred)\n",
    "        return di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MultiObjetiveOptimizer:\n",
    "    models = {\n",
    "        'LR_l2': LogisticRegression(penalty='l2', C=1.0, solver='liblinear', random_state=42),\n",
    "        'LR_l1': LogisticRegression(penalty='l1', C=1.0, solver='liblinear', random_state=42),\n",
    "        'RF_50': RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "        'RF_100': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'RF_150': RandomForestClassifier(n_estimators=150, random_state=42),\n",
    "        'DT_5': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "        'DT_10': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "        'DT_15': DecisionTreeClassifier(max_depth=15, random_state=42),\n",
    "        'MLP_50': MLPClassifier(hidden_layer_sizes=(50,), random_state=42),\n",
    "        'MLP_100': MLPClassifier(hidden_layer_sizes=(100,), random_state=42),\n",
    "        'MLP_150': MLPClassifier(hidden_layer_sizes=(150,), random_state=42),\n",
    "    }\n",
    "\n",
    "    def optimize(self, train, test, metric_evaluation_class: Callable, metric_monitor_classes: list[Callable]):\n",
    "        best_obj_score = np.inf\n",
    "        history = []\n",
    "        def objective_function(model, train, test):\n",
    "            metric = metric_evaluation_class(model, train, test) \n",
    "            return np.abs(metric.best_value - metric.compute())     \n",
    "\n",
    "        history = []\n",
    "        for model_name, model in tqdm(self.models.items()):\n",
    "            model = model\n",
    "            model.fit(train['X'], train['y'])\n",
    "            \n",
    "            obj_score = objective_function(model, train, test)\n",
    "             \n",
    "            if best_obj_score > obj_score:\n",
    "                best_obj_score = obj_score\n",
    "                best_model_name = model_name\n",
    "                best_model = model\n",
    "\n",
    "            scores = {m.__name__:m(model, train, test).compute() for m in metric_monitor_classes}\n",
    "            history.append({'model': model_name, 'obj_score':obj_score, **scores})\n",
    "\n",
    "        return {\n",
    "            'best_model_name': best_model_name,\n",
    "            'best_model': best_model,\n",
    "            'best_obj_score': best_obj_score,\n",
    "            'history': history\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = MultiObjetiveOptimizer()\n",
    "\n",
    "metric_evaluation_class = MyCustomMetric\n",
    "metric_monitor_classes = [EqualOpportunity, Accuracy, DisparateImpact]\n",
    "\n",
    "result = optimizer.optimize(train, test, metric_evaluation_class, metric_monitor_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['best_model_name'], result['best_obj_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(result['history'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_vs_bias_with_manual_colorbar(df):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6), sharey=False)\n",
    "    \n",
    "    # Scatter plot for Accuracy vs EqualOpportunity with color based on obj_score\n",
    "    sc1 = axes[0].scatter(df['Accuracy'], df['EqualOpportunity'], \n",
    "                          c=df['obj_score'], cmap='coolwarm', s=100)\n",
    "    axes[0].set_title('Accuracy vs Equal Opportunity')\n",
    "    axes[0].set_xlabel('Accuracy')\n",
    "    axes[0].set_ylabel('Equal Opportunity')\n",
    "    \n",
    "    # Annotate points with model names\n",
    "    for i, model in enumerate(df['model']):\n",
    "        axes[0].annotate(model, (df['Accuracy'][i], df['EqualOpportunity'][i]), fontsize=8, ha='right')\n",
    "\n",
    "    # Scatter plot for Accuracy vs DisparateImpact with color based on obj_score\n",
    "    sc2 = axes[1].scatter(df['Accuracy'], df['DisparateImpact'], \n",
    "                          c=df['obj_score'], cmap='coolwarm', s=100)\n",
    "    axes[1].set_title('Accuracy vs Disparate Impact')\n",
    "    axes[1].set_xlabel('Accuracy')\n",
    "    axes[1].set_ylabel('Disparate Impact')\n",
    "\n",
    "    # Annotate points with model names\n",
    "    for i, model in enumerate(df['model']):\n",
    "        axes[1].annotate(model, (df['Accuracy'][i], df['DisparateImpact'][i]), fontsize=8, ha='right')\n",
    "\n",
    "    # Adding color bar with substantial padding on the right of the figure\n",
    "    cbar = fig.colorbar(sc1, ax=axes, orientation='vertical', fraction=0.03)\n",
    "    cbar.set_label('Objective Score (obj_score)')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_vs_bias_with_manual_colorbar(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
